\section{Skalare Funktionen ($\mathbb{R}^n \rightarrow \mathbb{R}$)}

\subsection{Ableitungen}
\begin{tabular}{|p{5.3cm}|p{6.5cm}|p{6.5cm}|}
  \hline
    \textbf{partielle Ableitung\formelbuch{11/28}} & \textbf{Gradient\formelbuch{22/46}} & \textbf{Richtungsableitung\formelbuch{20/46}}\\
  \hline
    $\dprt{x}(x_0;y_0) := f_x(x_0;y_0)$ &
    
    $\gradient f(x_0,y_0)= \begin{pmatrix}
      f_x(x_0;y_0) \\ 
      f_y(x_0;y_0)
    \end{pmatrix} $ &
    
    An der Stelle $(x_0;y_0)$ \newline    
    $\dprt{\vec{r}}(x_0;y_0)=\gradient f(x_0;y_0) \bullet \vec{r}$ \\
  \hline
    $\fprt{^2f}{x^2}(x;y) = f_{xx}(x;y)$ \newline
    $\fprt{^2f}{x\partial y}(x;y) = f_{xy}(x;y) = f_{yx}(x;y)$ \newline
    $\fprt{^2f}{y^2}(x;y) = f_{yy}(x;y)$ &
    
    $\gradient f(x_1; \ldots;x_m) = \begin{pmatrix}
      f_{x_1}(x_1; \ldots;x_m)\\
      \vdots \\
      f_{x_m}(x_1; \ldots;x_m)
    \end{pmatrix}$ & 
    
    $\dprt{\vec{r}}(x_1, \ldots, x_m)=\gradient f(x_1, \ldots, x_m)
    \bullet \vec{r}$ \\
  \hline
    partielle Ableitung der Funktion $f$ nach dem Parameter $x_1$ &
    
    Vektor der partiellen Ableitungen von $f$. Der Gradient zeigt in Richtung der grössten Steigung und steht senkrecht auf der Niveaulinie.
    	Der Wert der Steigung entspricht $|\gradient f|$ &
    	
    	Steigung der Funktion $f$ in Richtung des Vektors 
    	$\vec{r} = \begin{pmatrix}
    	  \cos\alpha \\
    	  \sin\alpha
    	\end{pmatrix} (|\vec{r}|=1)$ \\
  \hline
\end{tabular}


\subsection{Tangentialebene}
  \begin{tabular}{|p{9cm}|p{9cm}|}
    \hline
      \textbf{2 Variablen\formelbuch{12}} &
      \textbf{m Variablen\formelbuch{44}} \\

      im Punkt: $x_0; y_0$ &
      im Punkt: $x_1^{(0)};\, \ldots \,; x_m^{(0)}$ \\
        
    \hline
      \[g(x,y)=f(x_0;y_0)+f_x(x_0;y_0)\cdot(x-x_0)+f_y(x_0;y_0)\cdot(y-y_0)\] &
      \begin{eqnarray}
        g(x_1;\ldots;x_m)  & = & f(x_1^{(0)};\ldots;x_m^{(0)}) + f_{x_1}(x_1^{(0)};\ldots;x_m^{(0)}) \cdot \nonumber \\
        & & (x_1-x_1^{(0)})+\ldots + f_{x_m}(x_1^{(0)};\ldots;x_m^{(0)}) \cdot \nonumber \\
        & & (x_m-x_m^{(0)}) \nonumber
      \end{eqnarray} \\
    \hline  
  \end{tabular}
  
\textbf{Für Approximation einer Formel}\\
Linearisierung am Punkt $(x_0;y_0)$: Tangentialebene $g(x;y)$ beim Punkt
$(x_0;y_0)$\\
Die Lösung des approximierten Wertes liefert dann $g(x;y)$.\\

$\boxed{\vec{n}(x_0,y_0)=
\begin{pmatrix}
	f_x(x_0,y_0)\\
	f_y(x_0,y_0)\\
	-1\\                         
\end{pmatrix}} \quad \Rightarrow \quad$ Normalenvektor der Tangentialebene

\subsection{Steigung\formelbuch{14}}
  $\boxed{m = -\frac{f_x(x_0;y_0)}{f_y(x_0;y_0)}}$ falls in einer impliziten Form $f(x;y) = 0$ und $f_y(x_0;y_0) \neq 0$
  

%\subsection{Niveaulinien und Falllinien\formelbuch{14}}
%\textbf{Falllinien, Orthogonaltrajektorien}\\
%Falllinien sind Kurven, die in jedem Punkt in Richtung max. Zunahme von f
%verlaufen\\
%$\Rightarrow \gradient f$ steht tangential zur Falllinie\\\\
%$\boxed{y'(x)= \dfrac{\prt{y}}{\prt{x}}} \quad
%\Rightarrow \quad$ Steigung der Falllinie $y(x)$\\\\
%Durch lösen dieser Differentialgleichung erhält man
%die Lösungskurve $y(x)$\\ \\
%\textbf{Niveaulinien}\\
%$\gradient f \perp $ Niveaulinie oder Tangentensteigung\\\\
%$\boxed{y'(x)=- \dfrac{\prt{x}}{\prt{y}}}\quad
%\Rightarrow \quad$ Steigung der Niveaulinie $y(x)$\\\\
%Durch lösen dieser Differentialgleichung erhält man
%die Lösungskurve $y(x)$. Auf diese Weise kann man auch einfach die
%Tangentensteigung einer impliziten Form z.B. $\frac{x^2}{100}+\frac{y^2}{25}=1$
%lösen

\subsection{Das totale Differenzial}

  \textbf{Die vier Grundoperationen} \\
  \begin{tabular}{|l|l|}
    \hline
      $z = f(x;y)$ &
      Standardabweichung \\
    \hline
      \begin{tabular}{l}
        $z = x + y$ \\
        $z = x - y$
      \end{tabular}  &    
      $\rbrace \sigma_{\bar z} \approx \sqrt{\sigma_{\bar x}^2 + \sigma_{\bar y}^2}$ \\
      
    \hline
      \begin{tabular}{l}
        $z = C\cdot x \cdot y$ \\
        $z = C\cdot \frac{x}{y}$
      \end{tabular} &
      $\rbrace |\frac{\sigma_{\bar z}}{\bar z}| \approx \sqrt{|\frac{\sigma_{\bar x}}{\bar x}|^2 + |\frac{\sigma_{\bar y}}{\bar y}|^2}$ \\
    \hline
  \end{tabular}
    

  \subsubsection{totale Differential}
    \begin{tabular}{|p{7cm}| l |p{10cm}| l}
      \cline{1-1}
      \cline{3-3}
        $\Delta z \approx dz=f_x(x_0;y_0)dx+f_y(x_0;y_0)dy$ &
        \formelbuch{16} &
        $\Delta f \approx df=f_{x_1}(x_1^{(0)};\ldots;x_m^{(0)})\cdot dx_1 + \ldots + f_{x_m}(x_1^{(0)};\ldots;x_m^{(0)})\cdot dx_m$ &
        \formelbuch{45} \\
      \cline{1-1}
      \cline{3-3}
    \end{tabular}      
  
  
  \subsubsection{Gauss'sches Fehlerfortpflanzungsgesetz}
    $\boxed{z=\bar z \pm \sigma_{\bar z} \approx f(\bar x, \bar y) \pm
    \sqrt{(f_x(\bar x,\bar y)\cdot \sigma_{\bar x})^2+(f_y(\bar x,\bar y)\cdot
    \sigma_{\bar y})^2}}$ \formelbuch{19} \\
    
    $\boxed{y=\bar y \pm \sigma_{\bar y} \approx f(\bar{x_1};\ldots;\bar{x_m}) \pm
    \sqrt{[f_{x_1}(\bar{x_1};\ldots;\bar{x_m}) \cdot \sigma_{\bar{x_1}}]^2 + \ldots + 
    [f_{x_m}(\bar{x_1};\ldots;\bar{x_m}) \cdot \sigma_{\bar{x_m}}]^2}}$ \formelbuch{45}
   

%\subsection{Zusammengesetzte Funktionen\formelbuch{34}}
%$\boxed{\gradient(g\circ f)(x)=g'(f(x))\cdot \gradient f(x)}$

\subsection{Kurvenapproximation}
  Punkte $P_1$ bis $P_n$ sind gegeben.
  \begin{enumerate}
    \item 
      Approximation durch eine Funktion: $y(x) = ax+b$\\
      $f(a;b) = \sum\limits_{i=1}^{n} (y(x_i)-y_i)^2 = \sum\limits_{i=1}^{n} (ax_i+b-y_i)^2$\\
    \item
      $f_a = \dprt{a} \sum\limits_{i=1}^{n} (ax_i+b-y_i)^2 = 2 \sum\limits_{i=1}^{n}x_i\cdot (ax_i+b-y_i)
      = 2 \sum\limits_{i=1}^{n}(ax_i^2+bx_i-x_iy_i)$ \\
      $f_b = \dprt{b} \sum\limits_{i=1}^{n} (ax_i+b-y_i)^2 = 2 \sum\limits_{i=1}^{n} (ax_i+b-y_i)$ \\
    \item
      $\gradient f = \left|\begin{matrix}
        f_a(a;b) \\
        f_b(a;b)
      \end{matrix} \right| = \left| \begin{matrix}
        2(a\bar{x^2} + b\bar{x} - \bar{xy}) \\
        2(a\bar{x} + b - \bar{y})
      \end{matrix} \right| = \left| \begin{matrix}
        a\bar{x^2} + b\bar{x} - \bar{xy} \\
        a\bar{x} + b - \bar{y}
      \end{matrix} \right| = \vec{0}$ \\
    \item
      Nach $a$ und $b$ auflösen und in die Funktion einsetzen
  \end{enumerate}
    
\subsection{numerische Verfahren zum Auffinden von stationären Punkten}
  \subsubsection{Newton Verfahren\formelbuch{29}}
    \begin{enumerate}
      \item
        Man startet mit einer möglichst guten Approximation $(x_0;y_0)$\\
      \item
        Man löst für $n = 0,1,2,\ldots$ das Gleichungssystem\\
        $\left|\begin{matrix}
          f_{xx}(x_n;y_n)\cdot\Delta x_n + f_{xy}(x_n;y_n)\cdot\Delta y_n = -f_x(x_n;y_n) \\
          f_{xy}(x_n;y_n)\cdot\Delta x_n + f_{yy}(x_n;y_n)\cdot\Delta y_n = -f_y(x_n;y_n)
        \end{matrix}\right|$ , und setze $(x_{n+1};y_{n+1}) := (x_n +\Delta x_n; y_n + \Delta y_n)$\\
      \item
        Den letzten berechneten Punkt $(x_{n+1};y_{n+1})$ akzeptiert man als Approximation für den stationären Punkt.
    \end{enumerate}
    
  \subsubsection{Gradientenverfahren \formelbuch{34}}
    Beim Gradientenverfahren startet man bei einer möglichst guten Approximation und folgt, dann immer der steilsten Steigung,
    bis genügend Konvergenz vorliegt.
    

\subsection{Extremalprobleme}
  \textbf{Zwei Variablen\formelbuch{25}}
  \begin{enumerate}
    \item 
      Randpunkte von $\mathbb{D}_f$
    \item 
      Punkte, in denen der Gradientenvektor $\gradient f$ nicht existiert. 
    \item 
      Punkte, in denen der Gradientenvektor $\gradient f=0$ ist.\\
      Hat die Funktion $f(x;y)$ an der Stelle $(x_0;y_0)$ einen verschwindenden Gradientenvektor $\gradient f = 0$ 
      und gilt für die Diskriminante $\Delta = f_{xx}    (x_0;y_0) \cdot f_{yy}(x_0;y_0) -  (f_{xy}(x_0;y_0))^2$
      \begin{itemize}
        \item 
          $\Delta > 0 $, so besitz $f(x_0;y_0)$ ein lokales Extremum. Im Fall $f_{xx}(x_0;y_0) < 0$ liegt ein lokales Maximum vor, 
          für $f_{xx}(x_0;y_0) > 0$ hingegen ein lokales Minimum.
        \item
          $\Delta < 0 $, so besitzt $f(x_0;y_0)$ in $(x_0;y_0)$ ein Sattelpunkt.
        \item 
          $\Delta = 0 $, so braucht es weitere Untersuchungen, um die Art der Stelle $(x_0;y_0)$ zu bestimmen können.
        \item 
          Wenn es kein lokales/relatives Minima/Maxima gibt, dann auch kein absolutes!
        \item
          Hat es ein lokales Maxima so gilt: $f(x;y) \leq M$ $\forall$ $(x;y) \in \mathbb{D}_f \Rightarrow M$ globales Maxima
      \end{itemize}
    \end{enumerate}
  
  \textbf{m Variablen\formelbuch{46}}
  Funktion $f(x_1^{(0)};\ldots;x_m^{(0)})$ gegeben.  \\
  \begin{itemize}
    \item 
      Schritte 1 - 3 sind gleich wie bei zwei Dimensionen. Kandidaten $(x_1^{(0)};\ldots;x_m^{(0)})$ bekommen.
    \item 
      Bestimmung der Art der Extremalstellen: Hessesche Matrix aufstellen\\
      $H(x_1^{(0)};\ldots;x_m^{(0)}) := \begin{pmatrix}
        f_{x_{1}x_{1}}(x_1^{(0)};\ldots;x_m^{(0)}) & \ldots & f_{x_{1}x_{m}}(x_1^{(0)};\ldots;x_m^{(0)})\\
        \vdots && \vdots\\
        f_{x_{m}x_{1}}(x_1^{(0)};\ldots;x_m^{(0)}) & \ldots & f_{x_{m}x_{m}}(x_1^{(0)};\ldots;x_m^{(0)})
      \end{pmatrix}$\\
      \begin{itemize}
        \item
          $m$ positive Eigenwerte $\lambda_i > 0$, so besitzt $f$ in $(x_1^{(0)};\ldots;x_m^{(0)})$ ein lokales Minimum,
        \item
          $m$ negative Eigenwerte $\lambda_i < 0$, so besitzt $f$ in $(x_1^{(0)};\ldots;x_m^{(0)})$ ein lokales Maximum,
        \item 
          positive und negative Eigenwerte, so besitzt $f$ in $(x_1^{(0)};\ldots;x_m^{(0)})$ einen Sattelpunkt.
        \item 
          Wenn $\lambda_i \leqq 0$ oder $\lambda_i \geqq 0$ sind weitere Untersuchungen nötig.
      \end{itemize}
  \end{itemize}

%\subsubsection{Extremstellen auf Randwerten \formelbuch{38}}
%\begin{tabular}{lll}
%	\begin{minipage}{3.5cm}
%		$\boxed{\gradient f - \lambda \gradient g = 0}$		
%	\end{minipage} &
%	\begin{minipage}{4.7cm}
%		$f: $ zu maximierende Funktion\\
%		$g: $ Funktion des Randes	
%    \end{minipage} &
%	\begin{minipage}{11cm}
%		1. Gleichung unter der Bedingung  $g=0$ auflösen\\
%		2. Lösungen durch einsetzen in $f$ auf Min o. Max untersuchen
%    \end{minipage}
%\end{tabular}

\subsection{Extremalprobleme mit Nebenbedingungen}
\begin{multicols}{2}
  \textbf{Zwei Variablen\formelbuch{42}}\\
  Gegeben: $f(x;y)$ unter der Nebenbedingung $n(x;y) = 0$\\
  So kommen folgende Punkte von $f$ in Frage:
  \begin{enumerate}
    \item 
      Randpunkte von $\mathbb{D}_f$ wenn sie die Nebenbedingungen  $n(x;y) = 0$ erfüllen und zu $\mathbb{D}_f$ gehören.
    \item 
      Punkte, in denen der Gradientenvektor $\gradient f $ und / oder $\gradient n$ nicht existieren, 
      und die Nebenbedingung
      $n(x;y) = 0$ erfüllen.
    \item 
      Lösungen des Gleichungsystems\\
      $ \begin{vmatrix}
        f_x(x;y) \cdot n_y(x;y) = f_y(x;y) \cdot n_x(x;y) \\
        n(x;y) = 0
     \end{vmatrix} $
   \item 
     Lösung in $f(x,y)$ einsetzen und untersuchen!
 \end{enumerate}
 \vfill
 
\columnbreak
  
 \textbf{m Variablen\formelbuch{49}}\\
 Falls eine Funktion $f(x_1;\ldots;x_m)$ unter den $k(<m)$ Nebenbedingungen $n_1(x_1;\ldots;x_m)=0, 
 \ldots, n_m(x_1;\ldots;x_m)=0$
 Maximal- oder Minimalstellen besitzt, kommen folgende Punkte in Frage:\\
 \begin{enumerate}
   \item
     Randpunkte von $\mathbb{D}_f$ falls sie zu $\mathbb{D}_f$ gehören und die Nebenbedingungen gleich 0 erfüllen.
   \item
     Punkte in denen (mindestens) einer der Gradientenvektoren $\gradient f, \gradient n_1, \ldots, \gradient n_k$ 
     nicht existieren
     oder in denen die $k$ Vektoren $\gradient n_1, \ldots, \gradient n_k$ linear abhängig sind und 
     die Nebenbedingungen erfüllen.
   \item
     Lösungen des $(m+k)\times(m+k)$ Gleichungssystems:\\
     $\begin{vmatrix}
       \gradient f(x_1;\ldots;x_m) & = & \lambda_1 \cdot \gradient n_1(x_1;\ldots;x_m)+ \ldots + \\
       & & +\lambda_k \cdot \gradient n_k(x_1;\ldots;x_m) \\
       n_1(x_1;\ldots;x_m) = 0 \\
       \vdots \\
       n_k(x_1;\ldots;x_m) = 0
     \end{vmatrix}$
   \item
     Lösungen in $f(x_1;\ldots;x_m)$ einsetzen.
 \end{enumerate}

\end{multicols}
